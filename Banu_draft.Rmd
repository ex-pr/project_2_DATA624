---
title: "Project 2: Predicting pH for ABC Beverage"
author: 'Banu Boopalan, Molly Siebecker, Marley Myrianthopoulos, Jonathan Burns, Daria Dubovskaia'
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
editor_options:
  chunk_output_type: console
  markdown:
    wrap: sentence
---

<style>
body {
    text-align: justify; /* Justifies text */
}
</style>

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
#chunks
knitr::opts_chunk$set(eval=TRUE, message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

#install.packages("FSAmisc")

#libraries
library(tidyverse)
library(summarytools)
library(fpp3)
library(readxl)
library(curl)
library(latex2exp)
library(seasonal)
library(GGally)
library(gridExtra)
library(reshape2)
library(Hmisc)
library(corrplot)
library(e1071)
library(caret)
library(VIM)
library(rpart)
library(forecast)
library(urca)
library(earth)
library(glmnet)
library(cluster)
library(kernlab)
library(aTSA)
library(AppliedPredictiveModeling)
library(mlbench)
library(randomForest)
library(party)
library(gbm)
library(Cubist)
library(partykit)
library(kableExtra)
library(factoextra)
library(FactoMineR)
library(naniar)
#library(FSAmisc)
```

<p align="center">
  <img src="https://raw.githubusercontent.com/ex-pr/project_2_DATA624/refs/heads/main/logo_abc.png">
</p>


# Overview

Assignment: This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel. Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

# 1. Data Exploration

## 1.1 Load data

```{r load_data}
#Load train data
#URL the raw .xlsx file
url <- "https://raw.githubusercontent.com/ex-pr/project_2_DATA624/main/StudentData.xlsx"
#Temporary path
temp_file <- tempfile(fileext = ".xlsx")
#Download file
curl_download(url, temp_file)
#Read file
data_beverage <- read_excel(temp_file)
#Copy data
beverage_df <- data_beverage
#Clean temp file
unlink(temp_file)

#Load test data
#URL the raw .xlsx file
url <- "https://raw.githubusercontent.com/ex-pr/project_2_DATA624/main/StudentEvaluation.xlsx"
#Temporary path
temp_file <- tempfile(fileext = ".xlsx")
#Download file
curl_download(url, temp_file)
#Read file
data_eval <- read_excel(temp_file)
#Copy data
eval_df <- data_eval
```

## 1.2 Summary Statistics

Beverage dataset: 2571 rows x 33 columns.  
Evaluation dataset: 267 rows x 33 columns.  
Mostly numeric, with one categorical variable (Brand Code).  

- NAs:  
Brand Code: ~4.7% missing in beverage_df, ~3.0% missing in eval_df.  
Many numeric features have a small percentage of missing values, generally < 2%.  
PH: 4 NAs, the target variable in beverage_df. eval_df has all values missing for PH (used for prediction).  
Impute missing values for numeric features using median or mean (or more advanced imputation if correlated features exist). Missing values should be handled within a robust pipeline to avoid information loss.  
Brand Code can be imputed or left as-is, depending on the percentage missing per brand.

- Skewness and outliers  
Variables like Mnf Flow have extreme values (mean is heavily influenced by outliers). Range: -100.2 to 229.4.  
PH: Check for extreme pH values that may affect model accuracy.  
Hyd Pressure1â€“4: High standard deviations (e.g., Hyd Pressure2 with a mean of ~21 and SD of 16.4).  
Analyze the distribution of these variables using histograms or boxplots. Maybe winsorization or BoxCox/log transformation for skewed distributions.

- Feature Importance for PH pred  
Carb Volume, Fill Ounces, PC Volume, Carb Pressure, and Carb Temp have small sd and are likely controlled manufacturing parameters. These might directly influence pH.  
Brand Code can be treated as a categorical predictor for brand-specific variations.  
Correlation or feature importance to identify which variables most strongly influence PH.  

- Brand Code: 4 levels (A, B, C, D).  
Unbalanced distribution: B accounts for ~50% of records, while A, C, and D are much smaller. The imbalance might affect models like decision trees or ensemble methods.  
Apply stratified sampling or weighting to handle imbalance during training. Explore interaction effects between Brand Code and numeric variables.

- Multicollinearity  
Variables such as Carb Volume, PC Volume, and Carb Pressure might be correlated due to their role in carbonation.  
Multiple pressure-related variables (Carb Pressure1, Fill Pressure, etc.) and filler speed/level metrics could also have collinear relationships.  
Compute a correlation matrix to detect highly correlated predictors.  
Principal Component Analysis (PCA) or Variance Inflation Factor (VIF) to handle multicollinearity.

- Data Leakage  
Variables like PSC, PSC Fill, and PSC CO could be downstream measures dependent on pH. Confirm whether these are part of the production process or outcome metrics.  
Analyze the production process to ensure no data leakage into the model.

- eval_df  
All PH values are missing for prediction. Maybe remove this column for now.  
Structure and missingness are similar to beverage_df. Ensure preprocessing and feature engineering pipelines are consistent between training and evaluation datasets.

- Feature engineering  
Ratios: Carb Volume / PC Volume (efficiency metrics)  
Differences: Carb Pressure - Fill Pressure (pressure loss)  
One-hot encoding for Brand Code  
Binning continuous variables (e.g., temperature ranges)  
I assume there is no timestamps, no need to add seasonality or shift-based features.

- Modeling Considerations  
Use feature scaling:variables like Carb Flow and Filler Speed have very different ranges and should be normalized or scaled for models like SVM or neural networks.  
If PH is skewed, maybe log or BoxCox transforms it for models sensitive to distribution (e.g., linear regression).

```{r summary_statistics}
#Check first rows of data
DT::datatable(
      beverage_df[1:10,],
      options = list(scrollX = TRUE,
                     deferRender = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     info = FALSE,  
                     paging=FALSE,
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 1: First 10 Rows of Beverage Data'
  )) 

DT::datatable(
      eval_df[1:10,],
      options = list(scrollX = TRUE,
                     deferRender = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     info = FALSE,      
                     paging=FALSE,
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 2: First 10 Rows of Evaluation Data'
  )) 

#Summary statistics
DT::datatable(
      dfSummary(beverage_df, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE),
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 3: Summary Statistics for Beverage Data'
  )) 
 

#Summary statistics
DT::datatable(
      dfSummary(eval_df, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE),
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 4: Summary Statistics for Evaluation Data'
  )) 

stat <- beverage_df %>%
  group_by(`Brand Code`) %>%
  filter(!is.na(`Brand Code`)) %>% 
  dplyr::summarize(
    Min = min(PH, na.rm = TRUE),
    Q1 = quantile(PH, 0.25, na.rm = TRUE),
    Median = median(PH, na.rm = TRUE),
    Mean = mean(PH, na.rm = TRUE),
    Q3 = quantile(PH, 0.75, na.rm = TRUE),
    Max = max(PH, na.rm = TRUE),
    StdDev = sd(PH, na.rm = TRUE),
    Count = n(),
    Missing = sum(is.na(PH)) # Add the count of NA values
  )

#Summary statistics by code
DT::datatable(
      stat,
      options = list(dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE,
                     paging=FALSE,
                     info = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 5: Summary Statistics for Each Brand Code'
  )) %>%
  DT::formatRound(columns = c("Min", "Q1", "Median", "Mean", "Q3", "Max", "StdDev"), digits = 3)
```


## 1.3 EDA 

The distribution of PH is roughly normal, centered around 8.5, with some outliers at the lower and upper tails. Gaussian distribution?  
Brand B has significantly more entries compared to others. Balancing or stratified sampling during model training?  
PH distribution across Brand Codes:  
Brand C has the lowest median PH, while Brand B has the highest.  
Outliers are present in all brand codes. Investigate whether these outliers are measurement errors or valid extreme cases.

```{r plots}
ggplot(beverage_df, aes(x = PH)) +
  geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of PH", x = "PH", y = "Frequency")

ggplot(beverage_df, aes(x = `Brand Code`)) +
  geom_bar(fill = "lightgreen") +
  theme_minimal() +
  labs(title = "Count of Entries by Brand Code", x = "Brand Code", y = "Count")

ggplot(beverage_df, aes(x = `Brand Code`, y = PH, fill = `Brand Code`)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Boxplot of PH by Brand Code", x = "Brand Code", y = "PH")

#ggplot(beverage_df, aes(x = `Carb Pressure`, y = PH)) +
#  geom_point(alpha = 0.6) +
#  theme_minimal() +
#  labs(title = "Scatterplot: Carb Pressure vs PH", x = "Carb Pressure", y = "PH")
```


MFR, Fill Speed, Hyd Pressure3, and Pressure Setpoint have outliers.  
PC Volume and PSC CO2 show categorical-like distributions.  
Mnf Flow has a long-tailed distribution.  
Filler Speed, Hyd Pressure2, and Usage Cont have sharp peaks.  
Hyd Pressure4 has an interesting spread, insight into process variability.  
MFR and Oxygen Filler are heavily right-skewed.  
Log or Box-Cox transformation for variables like MFR and Usage Cont to normalize.  
Group categorical-like numeric variables (e.g., PSC CO2, Carb Volume) to improve interpretability.  

```{r}
numeric_vars <- beverage_df %>% 
  dplyr::select(where(is.numeric)) %>% 
  names()

#3 groups
group_1 <- numeric_vars[1:10]
group_2 <- numeric_vars[11:20]
group_3 <- numeric_vars[21:32]

#Group 1 plot
beverage_df %>%
  dplyr::select(all_of(group_1)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.5, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 1")

# Group 2 Plot
beverage_df %>%
  dplyr::select(all_of(group_2)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.5, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 2")

# Group 3 Plot
beverage_df %>%
  dplyr::select(all_of(group_3)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.5, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 3")
```

The first two principal components explain approximately 20.16% and 18.74% of the variance. Some dimensionality reduction?  
Variables close to each other (e.g., Balling, Balling Lvl, Density, and Carb Rel) are strongly correlated. 
Dim1: Balling, Balling Lvl, Density, Carb Rel
Dim2: Hyd Pressure3, Carb Pressure1, Fill Speed, and PH.
Use them for modeling.  
Filler Level, Bowl Setpoint, and PH are grouped together, they may jointly influence the PH outcome.  
Mnf Flow and Hyd Pressure3 are distant, unique variance.  
Variables near the origin maybe drop.  
Combine strongly correlated (e.g., Carb Rel, Carb Flow, Density) or reduce using PCA components or clustering, factor analysis.  


```{r}
#PCA
pca <- PCA(beverage_df %>% dplyr::select(where(is.numeric)) %>% na.omit(), scale.unit = TRUE)
```


MFR highest percentage of missing values, around 7%. It is either a challenging variable to collect or might not be consistently relevant across all rows.  
mean, median, or a model-based imputation if MFR is critical.
If it does not strongly correlate with the target or other features, drop?  
When <3% missingness (PSC CO2, PC Volume, etc.), simple imputation techniques.  
For Brand Code, mode imputation.

```{r}
#NA
gg_miss_var(beverage_df, show_pct = TRUE)
```

```{r}
#Outliers
boxplot(beverage_df$PH, main = "Boxplot of PH", horizontal = TRUE)
```

```{r}
numeric_vars <- beverage_df %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(-PH) %>%
  names()

for (var in numeric_vars) {
  p <- ggplot(beverage_df, aes_string(x = paste0("`", var, "`"), y = "PH")) +
    geom_point(alpha = 0.6, color = "blue") +
    theme_minimal() +
    labs(title = paste("Scatterplot:", var, "vs PH"), x = var, y = "PH")
  
  print(p)  # Display the plot
}

```

## 1.4 Correlation Matrix

Bowl Setpoint (0.36) and Filler Level (0.35) show the strongest positive correlations with pH. These features might be significant predictors in any pH prediction model.  
Carb Flow (0.23) and Pressure Vacuum (0.22) also have moderate positive correlations with pH.  
Additional variables such as Carb Rel (0.20) and Alch Rel (0.17) show smaller but noteworthy correlations.  
Usage Cont (-0.36) and Mnf Flow (-0.46) have strong negative correlations with pH, indicating these might inversely impact pH levels.  
Other variables like Pressure Setpoint (-0.31) and Fill Pressure (-0.32) also negatively correlate with pH.  
Many variables such as Carb Volume, PSC, and PSC Fill have very weak correlations (close to 0), suggesting they might not be influential in predicting pH.

Density, Balling, and Carb Rel show strong intercorrelations among themselves. Multicollinearity issues in linear regression models. Maybe PCA or feature selection.

Mnf Flow and Usage Cont have strong negative correlations, maybe additional preprocessing or transformation.

Interaction terms between Bowl Setpoint and Filler Level or between Pressure Vacuum and Carb Flow could also be explored, given their individual relationships with pH.

```{r corr}
tst <- beverage_df %>% 
  dplyr::select(where(is.numeric))
#Correlation with PH
cor_table <- cor(drop_na(tst))[, "PH"] %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Coefficient = ".") %>%
  arrange(desc(Coefficient))

kable(cor_table, "html", escape = F, col.names = c('Variable', 'Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T) %>%
  add_header_above(c("Table: Correlation with PH" = 2))

#Corr matrix
rcore <- rcorr(as.matrix(tst))
coeff <- rcore$r
#Filter to include only |r| > 0.1, the rest are 0
filtered_coeff <- coeff
filtered_coeff[abs(filtered_coeff) < 0.1] <- 0 
coeff <- rcore$r
corrplot(filtered_coeff, tl.cex = .6, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust", number.cex=0.45, diag=FALSE)

#Heatmap maybe
#numeric_cols <- beverage_df %>%
#  select(where(is.numeric)) %>%
#  na.omit()

#correlation_matrix <- cor(numeric_cols)
#corrplot::corrplot(correlation_matrix, method = "color", type = "upper", diag = FALSE)
```


# 2. Data Transformation


```{r}
clean_bev <- beverage_df

nearzero <- nearZeroVar(clean_bev, saveMetrics= TRUE)
nearzero[nearzero$nzv,][1:5,] %>% drop_na()


```

MFR shows 7% missing - we'll remove this feature
Hyd Pressure1 shows near zero variance - we can remove this feature

```{r}
# Remove 2 fields
cleantrain <- clean_bev %>%
  dplyr::select(-c(MFR, `Hyd Pressure1`))

# remove the fields from our evaluation data
cleaneval <- eval_df %>%
  dplyr::select(-c(MFR, `Hyd Pressure1`))

cleaneval<- cleaneval%>%
  dplyr::select(-PH)

```

```{r}

set.seed(100)

# drop rows with missing PH
cleantrain <- cleantrain  %>%
  filter(!is.na(PH))

# Change Brand Code missing to 'Unknown' in our training data
brandcode <- cleantrain %>%
  dplyr::select(`Brand Code`) %>%
  replace_na(list(`Brand Code` = 'Unknown'))

cleantrain$`Brand Code` <- brandcode$`Brand Code`

# Change Brand Code missing to 'Unknown' 
brandcodeev <- cleaneval  %>%
  dplyr::select(`Brand Code`) %>%
  replace_na(list(`Brand Code` = 'Unknown'))

cleaneval$`Brand Code` <- brandcodeev$`Brand Code`

# change an unseen data as well
codes <- unique(cleantrain$`Brand Code`)

cleaneval <- cleaneval %>%
  mutate(`Brand Code`  = if_else(`Brand Code` %in% codes, `Brand Code`, 'Unknown'))

# Use the kNN imputing method from VIM package to impute missing values in our training data
cleantrain <- cleantrain %>% 
  kNN(k=10) %>%
  dplyr::select(colnames(cleantrain))

# # Use the kNN imputing method from VIM package to impute missing values in our training data
cleaneval<- cleaneval %>%
  kNN(k=10) %>%
  dplyr::select(colnames(cleaneval))

```

# Feature Engineering 

Add dummy variables for Brand Code

```{r}
cleantrain_dummy <- dummyVars(PH ~ `Brand Code`, data = cleantrain)
dummies <- predict(cleantrain_dummy, cleantrain)


dummy_cols <- sort(colnames(dummies))

# columns are sorted 
dummies <- as.tibble(dummies) %>%
  dplyr::select(dummy_cols)

# remove the original categorical feature
cleantrain <- cleantrain %>%
  dplyr::select(-`Brand Code`)

# add the new dummy columns to  train
cleantrain <- cbind(dummies, cleantrain)

# -----
# Evaluation data - Convert our `Brand Code` column into a set of dummy variables

cleaneval$PH <- 1
eval_dummies <- predict(cleantrain_dummy, cleaneval)

# if not found add 0

for (c in dummy_cols) {
  if (!(c %in% colnames(eval_dummies))) {
    eval_dummies[c] <- 0
  }
}


dummy_cols <- sort(colnames(eval_dummies))
eval_dummies <- as.tibble(eval_dummies) %>%
  dplyr::select(dummy_cols)

# remove the original categorical feature
cleaneval <- cleaneval %>%
  dplyr::select(-c(`Brand Code`, PH))

# add the new dummy columns to our main eval dataframe
cleaneval <- cbind(eval_dummies, cleaneval)
```

```{r}

library(caret)
preProcValues2 <- preProcess(cleantrain, method = "BoxCox")
train_bc <- predict(preProcValues2, cleantrain)

# preProcValues3 <- preProcess(cleaneval, method = "BoxCox")
# test_bc <- predict(preProcValues3, cleaneval )
# preProcValues2
# preProcValues3

```

```{r}
 #Prepare data for ggplot
gatherdf <- train_bc %>% 
  dplyr::select(-c(PH)) %>%
  gather(key = 'variable', value = 'value')

# Histogram plots of each variable
ggplot(gatherdf) + 
  geom_histogram(aes(x=value, y = after_stat(density)), bins=50) + 
  geom_density(aes(x=value), color='blue') +
  facet_wrap(. ~variable, scales='free', ncol=5)


```

# Build Models

- Start with Linear Regression after transformation and dimensionality reduction.
- Random Forest or Gradient Boosting for multicollinearity, so no dimensionality reduction but informed variable selection. 
-SVM  
- Regularized regression models, need to remove redundant features to reduce noise. 



```{r}
set.seed(100)

# utilizing one dataset for all four models
set <- createDataPartition(train_bc$PH, p=0.7, list=FALSE)
train <- train_bc[set,]
test <- train_bc[-set,]


```

# Model 1 - Linear Model on Train data

```{r}
set.seed(100)

#install.packages("doParallel")
library(doParallel)
library(MASS)

cl <- makePSOCKcluster(5)
registerDoParallel(cl)
set.seed(200)


model1_full_lm <- lm(PH ~ ., train )

# model1_full_lm  - (using stepAIC)
model1 <- stepAIC(model1_full_lm , direction = "both",
                         scope = list(upper = model1_full_lm , lower = ~ 1),
                         scale = 0, trace = FALSE)

stopCluster(cl)

# Display Model 1 Summary
(lms <- summary(model1))

confint(model1)

```

```{r}
set.seed(100)

library(car)
plot(model1)


```

```{r}
set.seed(100)
#install.packages("regclass")
library(regclass)

VIF(model1)

```

# Model 1 - Linear Model on Test data

```{r}
set.seed(100)

# Predict df_test and calculate performance
model1predict <- predict(model1, test)

# Merge the results into a data frame called results 
results <- data.frame()
results <- data.frame(t(postResample(pred = model1predict, obs = test$PH))) %>% 
  mutate(Model = "Mutiple Regression") %>% 
  rbind(results)

knitr::kable(results)
```

# Ridge Regression

```{r}

#install.packages("glmnet")
library(glmnet)
library(doParallel)
cl2 <- makePSOCKcluster(5)
registerDoParallel(cl2)
set.seed(200)

# to find the right lambda using cv.glmnet
xtrain <- model.matrix(PH ~ ., data = train)
xtest <- model.matrix(PH ~ ., data = test)

cv.glmnet <- cv.glmnet(xtrain, train$PH, alpha = 0)

ridgemdl <- glmnet(xtrain, train$PH, alpha = 0, lambda = cv.glmnet$lambda.min)

stopCluster(cl2)

summary(ridgemdl)



```

```{r}
# Predict df_test and calculate performance
ridge <- predict(ridgemdl, xtest)

results <- data.frame(t(postResample(pred = ridge, obs = test$PH))) %>% 
    dplyr::mutate(Model = "Ridge Regression") %>% rbind(results)

knitr::kable(results)

```

# Elastic Net

```{r}

cl4 <- makePSOCKcluster(5)
registerDoParallel(cl4)
set.seed(200)

# training the elastic net regression model using train
enet_model <- train(
                        PH ~ ., data = train, method = "glmnet",
                        trControl = trainControl("repeatedcv", repeats = 8),
                        tuneLength = 4
)

stopCluster(cl4)

summary(enet_model)

plot(enet_model)

```

```{r}
# Make predictions
enet_pred <- predict(enet_model, test)


results <- data.frame(t(postResample(pred=enet_pred, obs=test$PH))) %>% 
    mutate(Model = "ElasticNet Regression") %>% rbind(results)

knitr::kable(results)
```


# Neural Net (avNNET)

```{r}

cl5 <- makePSOCKcluster(5)
registerDoParallel(cl5)
set.seed(200)

# avvnet
nnetGrid <- expand.grid(.decay = c(0.1, 0.5), .size = c(1,10), .bag = FALSE)

nnet.mdl <- train(PH ~ ., data = train, method = "avNNet", preProcess = c("center", 
    "scale"), tuneGrid = nnetGrid, trControl = trainControl(method = "repeatedcv", 
    repeats = 1), trace = FALSE, linout = TRUE, maxit = 500)

stopCluster(cl5)

summary(nnet.mdl)

plot(nnet.mdl)
```


```{r}

# Predict df_test and calculate performance
nnetpred <- predict(nnet.mdl, newdata = test)
results <- data.frame(t(postResample(pred = nnetpred, obs = test$PH))) %>% 
    mutate(Model = "Neural Network (avNNET )") %>% rbind(results)

knitr::kable(results)

```

# KNN

```{r}

cl6 <- makePSOCKcluster(5)
registerDoParallel(cl6)
set.seed(200)

knnMdl <- train(PH ~ ., data = train, method = "knn", preProc = c("center","scale"), tuneLength = 10)

stopCluster(cl6)

summary(knnMdl)

plot(knnMdl)


```


```{r}
# Predict df_test and calculate performance
knn <- predict(knnMdl, newdata = test)
results <- data.frame(t(postResample(pred = knn, obs = test$PH))) %>% 
    mutate(Model = "k-Nearest Neighbors(kNN)") %>% rbind(results)

knitr::kable(results)
```

#GBM

```{r}

library(caret)
train1 <- train_bc %>% dplyr::select (-PH)

X.train <- train1[set, ]
y.train <- train_bc$PH[set]
X.test <- train1[-set, ]
y.test <- train_bc$PH[-set]

cl7 <- makePSOCKcluster(5)
registerDoParallel(cl7)
set.seed(200)

grid <- expand.grid(n.trees = c(50, 100, 150, 200),
                    interaction.depth = c(1, 5, 10, 15),
                    shrinkage = c(0.01, 0.1, 0.5),
    n.minobsinnode = c(5, 10, 15))

gbm_Mdl <- train(x = X.train,
                   y = y.train,
                   method = "gbm",
                   tuneGrid = grid,
                   verbose = FALSE
)

stopCluster(cl7)
summary(gbm_Mdl )



```

```{r}

plot(gbm_Mdl )
gbm_Mdl$bestTune
gbm_Mdl$finalModel

```

```{r}

# Predict df_test and calculate performance

gbm <- predict(gbm_Mdl, newdata = test)
# y_pred <- predict(xgb,  data.matrix(X.test[,-1]))
results <- data.frame(t(postResample(pred = gbm, obs = test$PH))) %>% mutate(Model = "Generalized Boosted Models") %>% rbind(results)

knitr::kable(results)

```


```{r}

options(max.print = 1e+06)

cl8 <- makePSOCKcluster(5)
registerDoParallel(cl8)
set.seed(200)

mars.grid <- expand.grid(.degree = 1:2, .nprune = 2:15)

mars.model <- train(x = X.train, y = y.train, method = "earth", tuneGrid = mars.grid, 
    preProcess = c("center", "scale"), tuneLength = 10)

stopCluster(cl8)

summary(mars.model)

```


```{r}

# Predict df_test and calculate performance
mars <- predict(mars.model, newdata = X.test)
results <- data.frame(t(postResample(pred = mars, obs = y.test))) %>% mutate(Model = "Multivariate Adaptive Reg
                                                                             ression Splines (MARS)") %>% rbind(results)

knitr::kable(results)

```


```{r}

cl9 <- makePSOCKcluster(5)
registerDoParallel(cl9)
set.seed(200)

cubist_Model <- train(x = X.train, y = y.train, method = "cubist")

stopCluster(cl9)


```



```{r}
# Predict df_test and calculate performance
Cubist <- predict(cubist_Model, newdata = X.test)
results <- data.frame(t(postResample(pred = Cubist, obs = y.test))) %>% mutate(Model = "Cubist Model") %>% rbind(results)

knitr::kable(results)
```


```{r}

set.seed(100)

library(randomForest)

rfModel <- randomForest(X.train, y.train,
                        importance = TRUE,
                        ntree = 1000)


rfPred <- predict(rfModel, X.test)

#postResample(rfPred, y.test)

results <- data.frame(t(postResample(pred = rfPred, obs = y.test))) %>% mutate(Model = "Random Forsest") %>% rbind(results)

knitr::kable(results)



```

```{r}
rfImp <- varImp(rfModel, scale = TRUE) %>%
  as.data.frame()

rfImp %>%
  arrange(-Overall) %>%
  kable() %>% 
  kable_styling() %>%
  scroll_box()
```

```{r}
varImpPlot(rfModel, sort = TRUE, n.var = 10)
```

```{r}
top10 <- varImp(rfModel) %>%
  filter(Overall < 57) %>%
  arrange(-Overall) %>%
  head(10)


cleantrain %>% 
  dplyr::select(c("PH", row.names(top10))) %>%
  cor() 

#%>% corrplot() + title("Correlation between PH and the Top 10 Numerical Variables")


```

```{r}

stat

```


```{r}

set.seed(100)
library(mice)
#install.packages("writexl")
library(writexl)

#head(test_bc)

#test_predict <- predict(cubist_Model , newdata=cleaneval)

#head(test_predict)

eval_data <- cleaneval %>% mutate(across(where(is.numeric), ~replace_na(., median(., na.rm=TRUE))))

ph_prediction <- predict(cubist_Model,eval_data)

eval_data$ph <- ph_prediction
head(eval_data)

write_xlsx(eval_data, 'StudentEvaluation_final.xlsx')

hist(eval_data$ph)
  


```
