---
title: "Project 2: Predicting pH for ABC Beverage"
author: 'Banu Boopalan, Molly Siebecker, Marley Myrianthopoulos, Jonathan Burns, Daria Dubovskaia'
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
editor_options:
  chunk_output_type: console
  markdown:
    wrap: sentence
---

<style>
body {
    text-align: justify; 
}
</style>

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
#chunks
knitr::opts_chunk$set(eval=TRUE, message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

#libraries
library(tidyverse)
library(summarytools)
library(fpp3)
library(readxl)
library(curl)
library(latex2exp)
library(seasonal)
library(GGally)
library(gridExtra)
library(reshape2)
library(Hmisc)
library(corrplot)
library(e1071)
library(caret)
library(VIM)
library(rpart)
library(forecast)
library(urca)
library(earth)
library(glmnet)
library(cluster)
library(kernlab)
library(aTSA)
library(AppliedPredictiveModeling)
library(mlbench)
library(randomForest)
library(party)
library(gbm)
library(Cubist)
library(partykit)
library(kableExtra)
library(factoextra)
library(FactoMineR)
library(naniar)
```

<p align="center">
  <img src="https://raw.githubusercontent.com/ex-pr/project_2_DATA624/refs/heads/main/logo_abc.png">
</p>


# Overview

Assignment: This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel. Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

# 1. Data Exploration

## 1.1 Load data

```{r load_data}
#Load train data
#URL the raw .xlsx file
url <- "https://raw.githubusercontent.com/ex-pr/project_2_DATA624/main/StudentData.xlsx"
#Temporary path
temp_file <- tempfile(fileext = ".xlsx")
#Download file
curl_download(url, temp_file)
#Read file
data_beverage <- read_excel(temp_file)
#Copy data
beverage_df <- data_beverage
#Clean temp file
unlink(temp_file)

#Load test data
#URL the raw .xlsx file
url <- "https://raw.githubusercontent.com/ex-pr/project_2_DATA624/main/StudentEvaluation.xlsx"
#Temporary path
temp_file <- tempfile(fileext = ".xlsx")
#Download file
curl_download(url, temp_file)
#Read file
data_eval <- read_excel(temp_file)
#Copy data
eval_df <- data_eval
```

## 1.2 Summary Statistics

Beverage dataset: 2571 rows x 33 columns.  
Evaluation dataset: 267 rows x 33 columns.  
Mostly numeric, with one categorical variable (Brand Code).  

- NAs:  
Brand Code: ~4.7% missing in beverage_df, ~3.0% missing in eval_df.  
Many numeric features have a small percentage of missing values, generally < 2%.  
PH: 4 NAs, the target variable in beverage_df. eval_df has all values missing for PH (used for prediction).  
Impute missing values for numeric features using median or mean (or more advanced imputation if correlated features exist). Missing values should be handled within a robust pipeline to avoid information loss.  
Brand Code can be imputed or left as-is, depending on the percentage missing per brand.

- Skewness and outliers  
Variables like Mnf Flow have extreme values (mean is heavily influenced by outliers). Range: -100.2 to 229.4.  
PH: Check for extreme pH values that may affect model accuracy.  
Hyd Pressure1–4: High standard deviations (e.g., Hyd Pressure2 with a mean of ~21 and SD of 16.4).  
Analyze the distribution of these variables using histograms or boxplots. Maybe winsorization or BoxCox/log transformation for skewed distributions.

- Feature Importance for PH pred  
Carb Volume, Fill Ounces, PC Volume, Carb Pressure, and Carb Temp have small sd and are likely controlled manufacturing parameters. These might directly influence pH.  
Brand Code can be treated as a categorical predictor for brand-specific variations.  
Correlation or feature importance to identify which variables most strongly influence PH.  

- Brand Code: 4 levels (A, B, C, D).  
Unbalanced distribution: B accounts for ~50% of records, while A, C, and D are much smaller. The imbalance might affect models like decision trees or ensemble methods.  
Apply stratified sampling or weighting to handle imbalance during training. Explore interaction effects between Brand Code and numeric variables.

- Multicollinearity  
Variables such as Carb Volume, PC Volume, and Carb Pressure might be correlated due to their role in carbonation.  
Multiple pressure-related variables (Carb Pressure1, Fill Pressure, etc.) and filler speed/level metrics could also have collinear relationships.  
Compute a correlation matrix to detect highly correlated predictors.  
Principal Component Analysis (PCA) or Variance Inflation Factor (VIF) to handle multicollinearity.

- Data Leakage  
Variables like PSC, PSC Fill, and PSC CO could be downstream measures dependent on pH. Confirm whether these are part of the production process or outcome metrics.  
Analyze the production process to ensure no data leakage into the model.

- eval_df  
All PH values are missing for prediction. Maybe remove this column for now.  
Structure and missingness are similar to beverage_df. Ensure preprocessing and feature engineering pipelines are consistent between training and evaluation datasets.

- Feature engineering  
Ratios: Carb Volume / PC Volume (efficiency metrics)  
Differences: Carb Pressure - Fill Pressure (pressure loss)  
One-hot encoding for Brand Code  
Binning continuous variables (e.g., temperature ranges)  
I assume there is no timestamps, no need to add seasonality or shift-based features.

- Modeling Considerations  
Use feature scaling:variables like Carb Flow and Filler Speed have very different ranges and should be normalized or scaled for models like SVM or neural networks.  
If PH is skewed, maybe log or BoxCox transforms it for models sensitive to distribution (e.g., linear regression).

```{r summary_statistics}
#Check first rows of data
DT::datatable(
      beverage_df[1:10,],
      options = list(scrollX = TRUE,
                     deferRender = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     info = FALSE,  
                     paging=FALSE,
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 1: First 10 Rows of Beverage Data'
  )) 

DT::datatable(
      eval_df[1:10,],
      options = list(scrollX = TRUE,
                     deferRender = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     info = FALSE,      
                     paging=FALSE,
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 2: First 10 Rows of Evaluation Data'
  )) 

#Summary statistics
DT::datatable(
      dfSummary(beverage_df, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE),
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 3: Summary Statistics for Beverage Data'
  )) 
 

#Summary statistics
DT::datatable(
      dfSummary(eval_df, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE),
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 4: Summary Statistics for Evaluation Data'
  )) 

stat <- beverage_df %>%
  group_by(`Brand Code`) %>%
  filter(!is.na(`Brand Code`)) %>% 
  dplyr::summarize(
    Min = min(PH, na.rm = TRUE),
    Q1 = quantile(PH, 0.25, na.rm = TRUE),
    Median = median(PH, na.rm = TRUE),
    Mean = mean(PH, na.rm = TRUE),
    Q3 = quantile(PH, 0.75, na.rm = TRUE),
    Max = max(PH, na.rm = TRUE),
    StdDev = sd(PH, na.rm = TRUE),
    Count = n(),
    Missing = sum(is.na(PH)) # Add the count of NA values
  )

#Summary statistics by code
DT::datatable(
      stat,
      options = list(dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE,
                     paging=FALSE,
                     info = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 5: Summary Statistics for Each Brand Code'
  )) %>%
  DT::formatRound(columns = c("Min", "Q1", "Median", "Mean", "Q3", "Max", "StdDev"), digits = 3)
```


## 1.3 EDA 

The pH of a solution is measured on a range of 0 to 14, indicating its acidity or alkalinity. The pH level in beverage manufacturing is crucial for flavor, safety, shelf life, and uniformity.

The most common pH in the dataset is around 8.5, indicating that this is the desired pH for many batches of beverages. For carbonated beverages, this slightly alkaline pH might:  
Improve Taste: Balance the tang of carbonation with a smoother, less acidic flavor.  
Aid in Shelf Stability: Make sure the beverage is resistant to microbial growth and chemical deterioration.  
Reflect Process Consistency: pH variations may reflect differences in raw material quality, carbonation levels, or filling precision.  

The distribution of PH is roughly normal, centered around 8.5, with some outliers at the lower and upper tails. Gaussian distribution?  
Brand B has significantly more entries compared to others. Balancing or stratified sampling during model training?  
PH distribution across Brand Codes:  
Brand C has the lowest median PH, while Brand B has the highest.  
Outliers are present in all brand codes. Investigate whether these outliers are measurement errors or valid extreme cases.

```{r plots}
ggplot(beverage_df, aes(x = PH)) +
  geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of PH", x = "PH", y = "Frequency")

ggplot(beverage_df, aes(x = `Brand Code`)) +
  geom_bar(fill = "lightgreen") +
  theme_minimal() +
  labs(title = "Count of Entries by Brand Code", x = "Brand Code", y = "Count")

ggplot(beverage_df, aes(x = `Brand Code`, y = PH, fill = `Brand Code`)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Boxplot of PH by Brand Code", x = "Brand Code", y = "PH")

#ggplot(beverage_df, aes(x = `Carb Pressure`, y = PH)) +
#  geom_point(alpha = 0.6) +
#  theme_minimal() +
#  labs(title = "Scatterplot: Carb Pressure vs PH", x = "Carb Pressure", y = "PH")
```


MFR, Fill Speed, Hyd Pressure3, and Pressure Setpoint have outliers.  
PC Volume and PSC CO2 show categorical-like distributions.  
Mnf Flow has a long-tailed distribution.  
Filler Speed, Hyd Pressure2, and Usage Cont have sharp peaks.  
Hyd Pressure4 has an interesting spread, insight into process variability.  
MFR and Oxygen Filler are heavily right-skewed.  
Log or Box-Cox transformation for variables like MFR and Usage Cont to normalize.  
Group categorical-like numeric variables (e.g., PSC CO2, Carb Volume) to improve interpretability.  

```{r}
numeric_vars <- beverage_df %>% 
  select(where(is.numeric)) %>% 
  names()

#3 groups
group_1 <- numeric_vars[1:10]
group_2 <- numeric_vars[11:20]
group_3 <- numeric_vars[21:32]

#Group 1 plot
beverage_df %>%
  select(all_of(group_1)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.5, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 1")

# Group 2 Plot
beverage_df %>%
  select(all_of(group_2)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.5, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 2")

# Group 3 Plot
beverage_df %>%
  select(all_of(group_3)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.5, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 3")
```

The first two principal components explain approximately 20.16% and 18.74% of the variance. Some dimensionality reduction?  
Variables close to each other (e.g., Balling, Balling Lvl, Density, and Carb Rel) are strongly correlated. 
Dim1: Balling, Balling Lvl, Density, Carb Rel
Dim2: Hyd Pressure3, Carb Pressure1, Fill Speed, and PH.
Use them for modeling.  
Filler Level, Bowl Setpoint, and PH are grouped together, they may jointly influence the PH outcome.  
Mnf Flow and Hyd Pressure3 are distant, unique variance.  
Variables near the origin maybe drop.  
Combine strongly correlated (e.g., Carb Rel, Carb Flow, Density) or reduce using PCA components or clustering, factor analysis.  


```{r}
#PCA
pca <- PCA(beverage_df %>% select(where(is.numeric)) %>% na.omit(), scale.unit = TRUE)
```


MFR highest percentage of missing values, around 7%. It is either a challenging variable to collect or might not be consistently relevant across all rows.  
mean, median, or a model-based imputation if MFR is critical.
If it does not strongly correlate with the target or other features, drop?  
When <3% missingness (PSC CO2, PC Volume, etc.), simple imputation techniques.  
For Brand Code, mode imputation.

```{r}
#NA
gg_miss_var(beverage_df, show_pct = TRUE)
```

```{r}
#Outliers
boxplot(beverage_df$PH, main = "Boxplot of PH", horizontal = TRUE)
```

```{r}
numeric_vars <- beverage_df %>%
  select(where(is.numeric)) %>%
  select(-PH) %>%
  names()

for (var in numeric_vars) {
  p <- ggplot(beverage_df, aes_string(x = paste0("`", var, "`"), y = "PH")) +
    geom_point(alpha = 0.6, color = "blue") +
    theme_minimal() +
    labs(title = paste("Scatterplot:", var, "vs PH"), x = var, y = "PH")
  
  print(p)  # Display the plot
}

```

## 1.4 Correlation Matrix

Bowl Setpoint (0.36) and Filler Level (0.35) show the strongest positive correlations with pH. These features might be significant predictors in any pH prediction model.  
Carb Flow (0.23) and Pressure Vacuum (0.22) also have moderate positive correlations with pH.  
Additional variables such as Carb Rel (0.20) and Alch Rel (0.17) show smaller but noteworthy correlations.  
Usage Cont (-0.36) and Mnf Flow (-0.46) have strong negative correlations with pH, indicating these might inversely impact pH levels.  
Other variables like Pressure Setpoint (-0.31) and Fill Pressure (-0.32) also negatively correlate with pH.  
Many variables such as Carb Volume, PSC, and PSC Fill have very weak correlations (close to 0), suggesting they might not be influential in predicting pH.

Density, Balling, and Carb Rel show strong intercorrelations among themselves. Multicollinearity issues in linear regression models. Maybe PCA or feature selection.

Mnf Flow and Usage Cont have strong negative correlations, maybe additional preprocessing or transformation.

Interaction terms between Bowl Setpoint and Filler Level or between Pressure Vacuum and Carb Flow could also be explored, given their individual relationships with pH.


Bowl Setpoint (correlation: 0.35).  
Role in Production: This parameter most likely affects the flow or intensity of ingredient mixing during the carbonation or flavoring stages.
Impact on pH: A higher bowl setpoint may result in more uniform mixing, lowering pH variability. Operators should keep a tight eye on this to avoid overmixing, which could result in deviations.  

Filler Level (correlation: 0.32).  
Role in Production: Ensures that containers are properly filled to prevent extra air or gas imbalances.
Impact on pH: Over- or under-filled containers may vary carbonation levels, influencing the final pH. Proper calibration reduces waste while preserving the beverage profile.

Pressure Vacuum (correlation: 0.22).  
Role in Production: Assists with gas exchange during filling.
Variations in vacuum pressure may cause inconsistent carbonation, changing the pH away from the target. Monitoring this ensures the desired "bite" of carbonation.

Oxygen Filler (correlation: 0.17).  
Role in Production: Determines the amount of oxygen introduced during filling.
Impact on pH: Higher oxygen levels may accelerate oxidation, reducing pH and compromising flavor stability. Maintaining a low oxygen level protects product quality.

Mnf Flow (correlation: -0.45)  
Role in Production: Describes the rate at which materials flow during production.
Impact on pH: A higher flow rate may result in irregular mixing, reduced carbonation, and a shift in pH. Adjusting flow rates can aid operators in maintaining consistent product attributes.




```{r corr}
tst <- beverage_df %>% 
  select(where(is.numeric))
#Correlation with PH
cor_table <- cor(drop_na(tst))[, "PH"] %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Coefficient = ".") %>%
  arrange(desc(Coefficient))

kable(cor_table, "html", escape = F, col.names = c('Variable', 'Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T) %>%
  add_header_above(c("Table: Correlation with PH" = 2))

#Corr matrix
rcore <- rcorr(as.matrix(tst))
coeff <- rcore$r
#Filter to include only |r| > 0.1, the rest are 0
filtered_coeff <- coeff
filtered_coeff[abs(filtered_coeff) < 0.1] <- 0 
coeff <- rcore$r
corrplot(filtered_coeff, tl.cex = .6, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust", number.cex=0.45, diag=FALSE)

#Heatmap maybe
#numeric_cols <- beverage_df %>%
#  select(where(is.numeric)) %>%
#  na.omit()

#correlation_matrix <- cor(numeric_cols)
#corrplot::corrplot(correlation_matrix, method = "color", type = "upper", diag = FALSE)
```


# 2. Data Preparation

- Missing Value Imputation  
Impute predictors using mean/mode or advanced methods (e.g., MICE).  
Ensure no missing values in the dataset before modeling.  
- Handle Outliers  
Winsorize or transform outliers to prevent skewed model behavior.  
- Encode Categorical Variables  
Use one-hot encoding for models requiring numerical inputs (e.g., linear regression, neural networks).  
Label encoding for tree-based models (e.g., random forest, XGBoost).  
Standardize or Normalize  
Standardize numerical variables for models sensitive to scale (e.g., linear regression, SVR, KNN).  
Scaling is not required for tree-based models.  
- Dimensionality Reduction  
Use PCA or feature selection (e.g., Recursive Feature Elimination) to reduce redundancy and multicollinearity.  

**Dimension reduction** exclude any near zero-variance predictors?  
near zero-variance: remove Hyd Pressure1


```{r}
# Identify zero-variance predictors
zero_var_1 <- nearZeroVar(beverage_df, saveMetrics = TRUE)
print(zero_var_1[zero_var_1$nzv, ])

beverage_df <- beverage_df[, !zero_var_predictors$nzv]
```

**Imputing missing values** mice() ?

NA brand code to brand code “A”? 

Change the Brand code to a factor variable

```{r}
preProcValues <- preProcess(beverage_df, method = c("medianImpute", "modeImpute"))
beverage_df_imputed <- predict(preProcValues, beverage_df)
eval_df_imputed <- predict(preProcValues, eval_df)
```

```{r}
#Outliers
num_cols <- names(beverage_df_imputed)[sapply(beverage_df_imputed, is.numeric)]
beverage_df_imputed[num_cols] <- beverage_df_imputed[num_cols] %>%
  mutate_all(~ Winsorize(., probs = c(0.05, 0.95)))
```

```{r}
#Encode Categorical Variables
beverage_df_imputed <- beverage_df_imputed %>%
  mutate(Brand.Code = as.factor(Brand.Code)) %>%
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.)) - 1))

#Standardize Numerical Predictors
num_cols <- names(beverage_df_imputed)[sapply(beverage_df_imputed, is.numeric)]
preProcScale <- preProcess(beverage_df_imputed[, num_cols], method = c("center", "scale"))
beverage_df_scaled <- predict(preProcScale, beverage_df_imputed )
eval_df_scaled <- predict(preProcScale, eval_df_imputed)
```


```{r}
#PCA to reduce dimensionality and handle multicollinearity
pca <- prcomp(beverage_df_scaled %>% select(num_cols), scale. = TRUE)
summary(pca) # Check explained variance

#Principal components explaining ~95% variance
pca_threshold <- 0.95
explained_var <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
num_components <- which(explained_var >= pca_threshold)[1]

#Transform datasets using selected principal components
beverage_df_pca <- as.data.frame(pca$x[, 1:num_components])
eval_df_pca <- as.data.frame(predict(pca, newdata = eval_df_scaled)[, 1:num_components])

#Add back the target variable
beverage_df_pca$PH <- beverage_df_scaled$PH
```

**Preprocess scale, center**



# 3. Build Models

```{r}
set.seed(547)
trainIndex <- createDataPartition(beverage_df_pca$PH, p = 0.8, list = FALSE)
train <- df1_scaled[trainIndex, ]
test <- df1_scaled[-trainIndex, ]

#Setup cv
control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
```

**Linear Model**

Preparation:  
Standardize numerical variables to ensure coefficients are comparable.  
Handle multicollinearity (remove highly correlated variables using PCA or VIF analysis).  
Encode categorical variables using one-hot encoding.  

```{r}
set.seed(547)
lm_model <- train(
  PH ~ ., data = train, method = "lm",
  trControl = control
)
summary(lm_model)
lm_pred <- predict(lm_model, test)
lm_rmse <- sqrt(mean((lm_pred - test$PH)^2))
print(paste("Linear Regression RMSE:", lm_rmse))
```


**Random Forest**

Robust against overfitting and captures complex interactions. Handles missing data better than most models.  

Preparation:  
Minimal scaling required.  
Impute missing values for predictors for improved accuracy.  

```{r}
set.seed(547)
rf_model <- train(
  PH ~ ., data = train, method = "rf",
  trControl = control,
  tuneLength = 5 
)
print(rf_model)
rf_pred <- predict(rf_model, test)
rf_rmse <- sqrt(mean((rf_pred - test$PH)^2))
print(paste("Random Forest RMSE:", rf_rmse))

rf_tune <- rf_model$results[rf_model$results$mtry == rf_model$bestTune$mtry, ]
rf_tune

rf_results <- postResample(pred = rf_pred, obs = test$PH)
rf_results
```

**Gradient Boosting**

Preparation:  
Ensure categorical variables are encoded appropriately (e.g., label encoding).  
Scaling is typically not required, but imputing missing values improves performance.  

```{r}
set.seed(547)
xgb_grid <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(2, 4, 6),
  eta = c(0.1, 0.3),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)
xgb_model <- train(
  PH ~ ., data = train, method = "xgbTree",
  trControl = control,
  tuneGrid = xgb_grid
)
print(xgb_model)
xgb_pred <- predict(xgb_model, test)
xgb_rmse <- sqrt(mean((xgb_pred - test$PH)^2))
print(paste("XGBoost RMSE:", xgb_rmse))
```


**Support Vector Regression**

Preparation:  
Standardize numerical predictors to avoid dominance by variables with larger scales.  
Encode categorical variables using one-hot encoding.  

```{r}
set.seed(547)
svr_model <- train(
  PH ~ ., data = train, method = "svmRadial",
  trControl = control,
  tuneLength = 5
)
print(svr_model)
svr_pred <- predict(svr_model, test)
svr_rmse <- sqrt(mean((svr_pred - test$PH)^2))
print(paste("SVR RMSE:", svr_rmse))
```


**Neural networks**

Preparation:  
Normalize or standardize numerical features.  
One-hot encode categorical variables.  
Impute missing values.  


```{r}
set.seed(547)
nn_model <- train(
  PH ~ ., data = train, method = "nnet",
  trControl = control,
  tuneLength = 5,
  linout = TRUE, # For regression
  trace = FALSE
)
print(nn_model)
nn_pred <- predict(nn_model, test)
nn_rmse <- sqrt(mean((nn_pred - test$PH)^2))
print(paste("Neural Network RMSE:", nn_rmse))

```

**K-Nearest Neighbors (KNN) Regression**

Preparation:  
Standardize or normalize numerical predictors for distance-based calculations.  
Impute missing values. 

```{r}
set.seed(547)
knn_model <- train(
  PH ~ ., data = train, method = "knn",
  trControl = control,
  tuneLength = 5
)
print(knn_model)
knn_pred <- predict(knn_model, test)
knn_rmse <- sqrt(mean((knn_pred - test$PH)^2))
print(paste("KNN RMSE:", knn_rmse))
```
