---
title: "Project 2: Predicting pH for ABC Beverage"
author: 'Banu Boopalan, Molly Siebecker, Marley Myrianthopoulos, Jonathan Burns, Daria Dubovskaia'
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
editor_options:
  chunk_output_type: console
  markdown:
    wrap: sentence
---

<style>
body {
    text-align: justify; 
}
</style>

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
#chunks
knitr::opts_chunk$set(eval=TRUE, message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

#libraries
library(tidyverse)
library(xgboost)
library(vip)
library(summarytools)
library(fpp3)
library(readxl)
library(curl)
library(latex2exp)
library(seasonal)
library(GGally)
library(gridExtra)
library(doParallel)
library(reshape2)
library(Hmisc)
library(corrplot)
library(e1071)
library(caret)
library(VIM)
library(rpart)
library(forecast)
library(urca)
library(earth)
library(glmnet)
library(cluster)
library(kernlab)
library(aTSA)
library(AppliedPredictiveModeling)
library(mlbench)
library(randomForest)
library(party)
library(gbm)
library(Cubist)
library(partykit)
library(kableExtra)
library(factoextra)
library(FactoMineR)
library(naniar)
library(mice)
library(janitor)
library(writexl)
```

<p align="center">
  <img src="https://raw.githubusercontent.com/ex-pr/project_2_DATA624/refs/heads/main/logo_abc.png">
</p>


# Overview

Assignment: This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel. Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

# 1. Data Exploration

## 1.1 Load data

```{r load_data}
#Load train data
#URL the raw .xlsx file
url <- "https://raw.githubusercontent.com/ex-pr/project_2_DATA624/main/StudentData.xlsx"
#Temporary path
temp_file <- tempfile(fileext = ".xlsx")
#Download file
curl_download(url, temp_file)
#Read file
data_beverage <- read_excel(temp_file)
#Copy data
beverage_df <- data_beverage
#Clean temp file
unlink(temp_file)

#Load test data
#URL the raw .xlsx file
url <- "https://raw.githubusercontent.com/ex-pr/project_2_DATA624/main/StudentEvaluation.xlsx"
#Temporary path
temp_file <- tempfile(fileext = ".xlsx")
#Download file
curl_download(url, temp_file)
#Read file
data_eval <- read_excel(temp_file)
#Copy data
eval_df <- data_eval
```

## 1.2 Summary Statistics

Beverage dataset: 2571 rows x 33 columns.  
Evaluation dataset: 267 rows x 33 columns.  
Mostly numeric, with one categorical variable (Brand Code).  

- NAs:  
Brand Code: ~4.7% missing in beverage_df, ~3.0% missing in eval_df.  
Many numeric features have a small percentage of missing values, generally < 2%.  
PH: 4 NAs for Brand code B, the target variable in beverage_df. eval_df has all values missing for PH (used for prediction).  
Impute missing values for numeric features using median or mean (or more advanced imputation if correlated features exist). Missing values should be handled within a robust pipeline to avoid information loss.  

- Skewness and outliers  
Variables like Mnf Flow have extreme values (mean is heavily influenced by outliers). Range: -100.2 to 229.4.  
PH: Check for extreme pH values that may affect model accuracy.  
Hyd Pressure1â€“4: High standard deviations (e.g., Hyd Pressure2 with a mean of ~21 and SD of 16.4).  
Analyze the distribution of these variables using histograms or boxplots. Maybe winsorization or BoxCox/log transformation for skewed distributions.

- Feature Importance for PH pred  
Carb Volume, Fill Ounces, PC Volume, Carb Pressure, and Carb Temp have small sd and are likely controlled manufacturing parameters. These might directly influence pH.  
Brand Code can be treated as a categorical predictor for brand-specific variations.  
Correlation or feature importance to identify which variables most strongly influence PH.  

- Brand Code: 4 levels (A, B, C, D).  
Unbalanced distribution: B accounts for ~50% of records, while A, C, and D are much smaller. The imbalance might affect models like decision trees or ensemble methods.  
Apply stratified sampling or weighting to handle imbalance during training. Explore interaction effects between Brand Code and numeric variables.

- Multicollinearity  
Variables such as Carb Volume, PC Volume, and Carb Pressure might be correlated due to their role in carbonation.  
Multiple pressure-related variables (Carb Pressure1, Fill Pressure, etc.) and filler speed/level metrics could also have collinear relationships.  
Compute a correlation matrix to detect highly correlated predictors.  
Principal Component Analysis (PCA) or Variance Inflation Factor (VIF) to handle multicollinearity.

- Data Leakage  
Variables like PSC, PSC Fill, and PSC CO could be downstream measures dependent on pH. Confirm whether these are part of the production process or outcome metrics.  
Analyze the production process to ensure no data leakage into the model.

- eval_df  
All PH values are missing for prediction. Maybe remove this column for now.  
Structure and missingness are similar to beverage_df. Ensure preprocessing and feature engineering pipelines are consistent between training and evaluation datasets.

- Feature engineering  
Ratios: Carb Volume / PC Volume (efficiency metrics)  
Differences: Carb Pressure - Fill Pressure (pressure loss)  
One-hot encoding for Brand Code  
Binning continuous variables (e.g., temperature ranges)  
I assume there is no timestamps, no need to add seasonality or shift-based features.

- Modeling Considerations  
Use feature scaling:variables like Carb Flow and Filler Speed have very different ranges and should be normalized or scaled for models like SVM or neural networks.  
If PH is skewed, maybe log or BoxCox transforms it for models sensitive to distribution (e.g., linear regression).

```{r summary_statistics}
#Check first rows of data
DT::datatable(
      beverage_df[1:10,],
      options = list(scrollX = TRUE,
                     deferRender = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     info = FALSE,  
                     paging=FALSE,
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 1: First 10 Rows of Beverage Data'
  )) 

DT::datatable(
      eval_df[1:10,],
      options = list(scrollX = TRUE,
                     deferRender = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     info = FALSE,      
                     paging=FALSE,
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 2: First 10 Rows of Evaluation Data'
  )) 

#Summary statistics
DT::datatable(
      dfSummary(beverage_df, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE),
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 3: Summary Statistics for Beverage Data'
  )) 
 

#Summary statistics
DT::datatable(
      dfSummary(eval_df, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE),
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 4: Summary Statistics for Evaluation Data'
  )) 

stat <- beverage_df %>%
  group_by(`Brand Code`) %>%
  filter(!is.na(`Brand Code`)) %>% 
  dplyr::summarize(
    Min = min(PH, na.rm = TRUE),
    Q1 = quantile(PH, 0.25, na.rm = TRUE),
    Median = median(PH, na.rm = TRUE),
    Mean = mean(PH, na.rm = TRUE),
    Q3 = quantile(PH, 0.75, na.rm = TRUE),
    Max = max(PH, na.rm = TRUE),
    StdDev = sd(PH, na.rm = TRUE),
    Count = n(),
    Missing = sum(is.na(PH)) 
  )

#Summary statistics by code
DT::datatable(
      stat,
      options = list(dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE,
                     paging=FALSE,
                     info = FALSE), 
      rownames = FALSE,
      caption = htmltools::tags$caption(
    style = 'caption-side: top; text-align: left; font-size: 16px; font-weight: bold;',
    'Table 5: Summary Statistics PH for Each Brand Code'
  )) %>%
  DT::formatRound(columns = c("Min", "Q1", "Median", "Mean", "Q3", "Max", "StdDev"), digits = 3)
```


## 1.3 EDA 

Carb Volume: Volume of the beverage that is carbonated.  
Fill Ounces: Volume of the beverage filled into containers, measured in ounces.  
PC Volume: Could indicate "Process Control Volume," a measure related to production parameters.  
Carb Pressure: Pressure used during the carbonation process.  
Carb Temp: Temperature of the beverage during carbonation.  
PSC: Potentially a quality control metric, the exact meaning would depend on industry context.  
PSC Fill: Quality or quantity metric related to the filling process.  
PSC CO2: Measures CO2 levels or pressure during processing.  
Mnf Flow: Manufacturing flow rate, likely related to how fast the production line operates.  
Carb Pressure1: Another measurement of pressure during carbonation, possibly at a different production stage.  
Fill Pressure: Pressure maintained during the filling of beverages into containers.  
Hyd Pressure1, Hyd Pressure2, Hyd Pressure3, Hyd Pressure4: Different points of hydraulic pressure measurement within the machinery.  
Filler Level: Level to which containers are filled; critical for consistency.  
Filler Speed: Speed at which the filling machinery operates.  
Temperature: General temperature measurement.  
Carb Flow: Flow rate of the carbonation process.  
Density: Physical density of the beverage, which impacts taste and mouthfeel.  
MFR: Manufacturerâ€™s reference; could relate to specific production settings or machine identifiers.  
Balling: Measurement related to the sugar content of a liquid, typically used in brewing.    
Pressure Vacuum: Pressure readings from a vacuum process, possibly in packaging.  
Oxygen Filler: Likely measures the presence of oxygen during filling, which must be minimized in certain beverages.  
Bowl Setpoint: Setpoint for a particular part of the machinery, possibly related to mixing or blending.  
Pressure Setpoint: Target pressure to be maintained within certain machinery or process stages.  
Air Pressurer: Air pressure readings.  
Alch Rel: Could be "Alcohol Release" or related to the alcohol content management.  
Carb Rel: Possibly "Carbonation Reliability" or a similar measure of carbonation consistency.  
Balling Lvl: Specific gravity of the liquid related to its sugar content, important in quality control in brewing and beverage manufacturing.  

The pH of a solution is measured on a range of 0 to 14, indicating its acidity or alkalinity. The pH level in beverage manufacturing is crucial for flavor, safety, shelf life, and uniformity.

The most common pH in the dataset is around 8.5, indicating that this is the desired pH for many batches of beverages. For carbonated beverages, this slightly alkaline pH might:  
Improve Taste: Balance the tang of carbonation with a smoother, less acidic flavor.  
Aid in Shelf Stability: Make sure the beverage is resistant to microbial growth and chemical deterioration.  
Reflect Process Consistency: pH variations may reflect differences in raw material quality, carbonation levels, or filling precision.  

The distribution of PH is roughly normal (slight negative skew), centered around 8.5, with some outliers at the lower and upper tails. 

Brand B has significantly more entries compared to others. Balancing or stratified sampling during model training? 

PH distribution across Brand Codes:  
Brand C has the lowest median PH, while Brand B has the highest.  
Outliers are present in all brand codes. Investigate whether these outliers are measurement errors or valid extreme cases.

```{r plots}
ggplot(beverage_df, aes(x = PH)) +
  geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of PH", x = "PH", y = "Frequency")

ggplot(beverage_df, aes(x = `Brand Code`)) +
  geom_bar(fill = "lightgreen") +
  theme_minimal() +
  labs(title = "Count by Brand Code", x = "Brand Code", y = "Count")

filtered_df <- beverage_df %>%
  filter(!is.na(`Brand Code`))

ggplot(filtered_df, aes(x = `Brand Code`, y = PH, fill = `Brand Code`)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Boxplot of PH by Brand Code", x = "Brand Code", y = "PH")
```


MFR, Fill Speed, Hyd Pressure3, and Pressure Setpoint have outliers.  
Alch Rel, Balling, Balling Lvl have bimonial distribution, maybe binning will be needed.  
Mnf Flow has a long-tailed distribution.  
Filler Speed, Hyd Pressure2, and Usage Cont have sharp peaks.  
Hyd Pressure4 has an interesting spread, insight into process variability.  
MFR and Oxygen Filler are heavily right-skewed.  
Log or Box-Cox transformation for variables like MFR and Usage Cont to normalize.  

```{r}
#numeric_vars <- beverage_df %>% 
#  select(where(is.numeric)) %>% 
#  select(-PH) %>% 
#  names() 
#3 groups
#group_1 <- numeric_vars[1:10]
#group_2 <- numeric_vars[11:20]
#group_3 <- numeric_vars[21:31]


group_1 <- c("Carb Pressure", "Carb Pressure1", "Carb Temp", "Temperature", 
             "Usage cont", "Fill Pressure", "Air Pressurer", "Alch Rel", 
             "Balling", "Balling Lvl")

group_2 <- c("Carb Volume", "Carb Rel", "Fill Ounces", "Oxygen Filler", "Density", 
             "PC Volume", "PSC", "PSC CO2", 
             "PSC Fill", "Pressure Setpoint", "Pressure Vacuum")

group_3 <- c("Mnf Flow", "Carb Flow", "Filler Level", "Filler Speed", "Hyd Pressure1", 
             "Hyd Pressure2", "Hyd Pressure3", "Hyd Pressure4", "MFR", "Bowl Setpoint")

#Group 1 plot
beverage_df %>%
  select(all_of(group_1)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.3, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 1")

#Carb Pressure, Carb Pressure 1, Carb Temp, Temperature, Usage cont, Fill Pressure, Air Pressurer, Alch Rel trinomial, Balling binomial, Balling Lvl binomial

#Carb Volume, Carb Rel, Fill Ounces, Oxygen Filler, Density, PC Volume normal, PSC skew right, PSC CO2 skew right, PSC Fill skew right, Pressure Setpoint, Pressure Vacuum

#Mnf Flow, Carb Flow, Filler Level, Filler Speed, Hyd Pressure1, Hyd Pressure2, Hyd Pressure3, Hyd Pressure4, MFR, Bowl Setpoint, MFR


# Group 2 Plot
beverage_df %>%
  select(all_of(group_2)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 0.05, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 2")

# Group 3 Plot
beverage_df %>%
  select(all_of(group_3)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  facet_wrap(~name, scales = "free", ncol = 5) +
  theme_minimal() +
  labs(title = "Distribution of Variables: Group 3")
```

MFR highest percentage of missing values, around 7%. It is either a challenging variable to collect or might not be consistently relevant across all rows.  
mean, median, or a model-based imputation if MFR is critical.
If it does not strongly correlate with the target or other features, drop?  
When <3% missingness (PSC CO2, PC Volume, etc.), simple imputation techniques.  
For Brand Code, mode imputation.

```{r}
#NA
gg_miss_var(beverage_df, show_pct = TRUE)
```

```{r}
for (var in numeric_vars) {
  p <- ggplot(beverage_df, aes_string(x = paste0("`", var, "`"), y = "PH")) +
    geom_point(alpha = 0.6, color = "blue") +
    theme_minimal() +
    labs(title = paste("Scatterplot:", var, "vs PH"), x = var, y = "PH")
  
  print(p)  # Display the plot
}

```

## 1.4 Correlation Matrix


Density, Balling, and Carb Rel show strong intercorrelations among themselves. Multicollinearity issues in linear regression models. Maybe PCA or feature selection.

Mnf Flow and Usage Cont have strong negative correlations, maybe additional preprocessing or transformation.

Interaction terms between Bowl Setpoint and Filler Level or between Pressure Vacuum and Carb Flow could also be explored, given their individual relationships with pH.


Bowl Setpoint (correlation: 0.36).  
Role in Production: This parameter most likely affects the flow or intensity of ingredient mixing during the carbonation or flavoring stages.
Impact on pH: A higher bowl setpoint may result in more uniform mixing, lowering pH variability. Operators should keep a tight eye on this to avoid overmixing, which could result in deviations.  

Filler Level (correlation: 0.35).  
Role in Production: Ensures that containers are properly filled to prevent extra air or gas imbalances.
Impact on pH: Over- or under-filled containers may vary carbonation levels, influencing the final pH. Proper calibration reduces waste while preserving the beverage profile.

Pressure Vacuum (correlation: 0.22).  
Role in Production: Assists with gas exchange during filling.
Variations in vacuum pressure may cause inconsistent carbonation, changing the pH away from the target. Monitoring this ensures the desired "bite" of carbonation.

Oxygen Filler (correlation: 0.17).  
Role in Production: Determines the amount of oxygen introduced during filling.
Impact on pH: Higher oxygen levels may accelerate oxidation, reducing pH and compromising flavor stability. Maintaining a low oxygen level protects product quality.

Mnf Flow (correlation: -0.45)  
Role in Production: Describes the rate at which materials flow during production.
Impact on pH: A higher flow rate may result in irregular mixing, reduced carbonation, and a shift in pH. Adjusting flow rates can aid operators in maintaining consistent product attributes.




```{r corr}
tst <- beverage_df %>% 
  select(where(is.numeric))
#Correlation with PH
cor_table <- cor(drop_na(tst))[, "PH"] %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Coefficient = ".") %>%
  arrange(desc(Coefficient))

kable(cor_table, "html", escape = F, col.names = c('Variable', 'Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T) %>%
  add_header_above(c("Table: Correlation with PH" = 2))

#Corr matrix
rcore <- rcorr(as.matrix(tst))
coeff <- rcore$r
#Filter to include only |r| > 0.1, the rest are 0
filtered_coeff <- coeff
filtered_coeff[abs(filtered_coeff) < 0.1] <- 0 
coeff <- rcore$r
corrplot(filtered_coeff, tl.cex = .6, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust", number.cex=0.45, diag=FALSE)
```


# 2. Data Preparation

- Missing Value Imputation  
Impute predictors using mean/mode or advanced methods (e.g., MICE).  
Ensure no missing values in the dataset before modeling.  

- Handle Outliers  
Winsorize or transform outliers to prevent skewed model behavior.  
- Encode Categorical Variables  

Use one-hot encoding for models requiring numerical inputs (e.g., linear regression, neural networks).  
Label encoding for tree-based models (e.g., random forest, XGBoost).  

Standardize or Normalize  
Standardize numerical variables for models sensitive to scale (e.g., linear regression, SVR, KNN).  
Scaling is not required for tree-based models.  

- Dimensionality Reduction  
Use PCA or feature selection (e.g., Recursive Feature Elimination) to reduce redundancy and multicollinearity.  

**Dimension reduction** exclude any near zero-variance predictors?  
near zero-variance: remove Hyd Pressure1


Fix colnames
```{r}
#Fix column names
colnames(beverage_df) <- make_clean_names(colnames(beverage_df))

#Check new column names
colnames(beverage_df)

#Apply the same to eval_df 
colnames(eval_df) <- make_clean_names(colnames(eval_df))

colnames(eval_df)
```


```{r}
# Identify zero-variance predictors
zero_var <- nearZeroVar(beverage_df, saveMetrics = TRUE)
print(zero_var[zero_var$nzv, ])

beverage_df <- beverage_df[, !zero_var$nzv]
eval_df <- eval_df[, !zero_var$nzv]
```

**Imputing missing values** mice() ?

For Brand Code random forest imputation, for the rest - mice.

```{r}
set.seed(547)
#Exclude Brand Code from the imputation
beverage_mice <- beverage_df %>% select(-brand_code)
eval_mice <- eval_df %>% select(-brand_code)

#Apply MICE to train df
imputed_train <- mice(beverage_mice, method='pmm', m=5, seed=547)

#Extract the completed train df
completed_train <- complete(imputed_train, 1)

# Apply the same imputation model to evaluation data
imputed_eval <- mice(eval_mice, method='pmm', m=5, seed=123)
completed_eval <- complete(imputed_eval, 1)

#Add the imputed columns back
beverage_imputed <- cbind(completed_train, brand_code = beverage_df$brand_code)
eval_imputed <- cbind(completed_eval, brand_code = eval_df$brand_code)
```

```{r}
set.seed(547)
#Separate data with and without missing Brand Code
df_complete <- beverage_imputed[!is.na(beverage_imputed$brand_code), ]
df_missing <-beverage_imputed[is.na(beverage_imputed$brand_code), ]

df_eval_complete <- eval_imputed[!is.na(eval_imputed$brand_code), ]
df_eval_missing <- eval_imputed[is.na(eval_imputed$brand_code), ]

df_complete$brand_code <- as.factor(df_complete$brand_code)
df_eval_complete$brand_code <- as.factor(df_eval_complete$brand_code)

#Train Random Forest
rf_model <- randomForest(brand_code ~ ., data=df_complete, ntree=100)

#Predict missing Brand Codes
df_missing$brand_code <- predict(rf_model, newdata=df_missing)
df_eval_missing $brand_code <- predict(rf_model, newdata=df_eval_missing)

#Combine back
beverage_imputed <- rbind(df_complete, df_missing)
eval_imputed <- rbind(df_eval_complete, df_eval_missing)
```

**One hot encoding, interaction terms**

```{r}
#One-hot Encoding for Brand Code
beverage_imputed$brand_code <- as.factor(beverage_imputed$brand_code)
eval_imputed$brand_code <- as.factor(eval_imputed$brand_code)
dummy_vars <- dummyVars(~ brand_code, data = beverage_imputed, fullRank = TRUE)
beverage_imputed <- cbind(beverage_imputed, predict(dummy_vars, newdata = beverage_imputed))
beverage_imputed <- beverage_imputed %>% select(-brand_code) 
eval_imputed <- cbind(eval_imputed, predict(dummy_vars, newdata = eval_imputed))
eval_imputed <- eval_imputed %>% select(-brand_code) 


#Creating Interaction Terms
beverage_featured <- beverage_imputed %>%
  mutate(
    carbonation_compos = (balling + balling_lvl + density + alch_rel + carb_rel) / 5,
    pressure_temp = carb_pressure * carb_temp,
    gas_control_indx = (psc + psc_fill + psc_co2) / 3,
    setpoint_diff = bowl_setpoint - pressure_setpoint,
    filler_efficiency = filler_speed * mfr,
    hyd_pressure_indx = (hyd_pressure2 + hyd_pressure3 + hyd_pressure4) / 3
    
  ) %>%
  select(-c(filler_speed, mfr,balling, balling_lvl, density, alch_rel, carb_rel, carb_pressure, carb_temp, psc, psc_fill, psc_co2, bowl_setpoint, pressure_setpoint, hyd_pressure2, hyd_pressure3, hyd_pressure4))



eval_featured <- eval_imputed %>%
  mutate(
    carbonation_compos = (balling + balling_lvl + density + alch_rel + carb_rel) / 5,
    pressure_temp = carb_pressure * carb_temp,
    gas_control_indx = (psc + psc_fill + psc_co2) / 3,
    setpoint_diff = bowl_setpoint - pressure_setpoint,
    filler_efficiency = filler_speed * mfr,
    hyd_pressure_indx = (hyd_pressure2 + hyd_pressure3 + hyd_pressure4) / 3
  ) %>%
  select(-c(filler_speed, mfr,balling, balling_lvl, density, alch_rel, carb_rel, carb_pressure, carb_temp, psc, psc_fill, psc_co2, bowl_setpoint, pressure_setpoint, hyd_pressure2, hyd_pressure3, hyd_pressure4))

#MAYBE????
#remove_vars <- c('mnf_flow')
#name_vars <- names(beverage_featured) %in% remove_vars

# Dropping highly correlated variables.
#beverage_vars <- beverage_featured[,!name_vars]  

#name_vars <- names(eval_featured) %in% remove_vars
#eval_vars <- eval_featured[,!name_vars]  
```

```{r}
#MAYBE????
#remove_vars <- c('mnf_flow', 'hyd_pressure2', 'hyd_pressure3', 'density', 'alch_rel', 'balling_lvl', 'filler_level')
#remove_vars <- c('mnf_flow')
#name_vars <- names(beverage_imputed) %in% remove_vars

# Dropping highly correlated variables.
#beverage_vars <- beverage_imputed[,!name_vars]  

#name_vars <- names(eval_imputed) %in% remove_vars
#eval_vars <- eval_imputed[,!name_vars] 
```


# 3. Build Models

```{r}
# Detect the number of available cores
numCores <- detectCores()

# Register parallel backend (use numCores - 1 to leave 1 core free for system tasks)
cl <- makeCluster(numCores - 1)
registerDoParallel(cl)

# Verify the setup
getDoParWorkers()  # This should return the number of cores in use
```

```{r}
#set.seed(547)
set.seed(123)
trainIndex <- createDataPartition(beverage_imputed$ph, p = 0.8, list = FALSE)
train_imputed <- beverage_imputed[trainIndex, ]
test_imputed <- beverage_imputed[-trainIndex, ]

#Setup cv
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel = TRUE)  #verboseIter = TRUE

#Remove PH column from eval_df
eval_imputed <- subset(eval_imputed, select = -ph)
```

```{r}
#Data with new features
#set.seed(547)
set.seed(123)
trainIndex <- createDataPartition(beverage_featured$ph, p = 0.8, list = FALSE)
train_featured <- beverage_featured[trainIndex, ]
test_featured <- beverage_featured[-trainIndex, ]

#Remove PH column from eval_df
eval_featured <- subset(eval_featured, select = -ph)
```

```{r}
#Data with new features
#set.seed(547)
#trainIndex <- createDataPartition(beverage_vars$ph, p = 0.8, list = FALSE)
#train_vars <- beverage_vars[trainIndex, ]
#test_vars <- beverage_vars[-trainIndex, ]

#Remove PH column from eval_df
#eval_vars <- subset(eval_vars, select = -ph)
```


**Random Forest**

Robust against overfitting and captures complex interactions. Handles missing data better than most models.  

Preparation:  
Minimal scaling required.  
Impute missing values for predictors for improved accuracy.  
can handle multicollinearity 

```{r}
#set.seed(547)
set.seed(123)
rfGrid <- expand.grid(.mtry = c(2, 4, 6, 8, 10))
rf_model <- train(ph ~ ., data = train_imputed, 
                  method = "rf",
                  preProc = c("center", "scale"),
                  trControl = control,
                  tuneGrid = rfGrid,
                  importance = TRUE) 

#mtry=10, RMSE=0.1012154, R2=0.6745374, MAE=0.07393928
rf_tune <- rf_model$results[rf_model$results$mtry == rf_model$bestTune$mtry, ]
rf_tune

rf_pred <- predict(rf_model, newdata = test_imputed)

#RMSE=0.10027544, R2=0.69053772 MAE= 0.07212886
rf_results <- postResample(pred = rf_pred, obs = test_imputed$ph)
rf_results
```

Variable importance

                Overall
mnf_flow          40.10
pressure_vacuum   36.07
oxygen_filler     35.14
brand_code.C      32.60
usage_cont        27.39
carb_rel          26.99
balling_lvl       26.54
air_pressurer     25.79
temperature       25.17
filler_speed      24.66
alch_rel          23.17
balling           22.57
carb_pressure1    21.40
carb_flow         21.13
bowl_setpoint     20.24
density           20.23
filler_level      17.84
hyd_pressure3     17.00
mfr               16.89
carb_volume       16.47

```{r}
importance_rf <- varImp(rf_model, scale = FALSE)
importance_rf

plot(importance_rf, top = 10, main = "top 10 predictors")
```

**Gradient Boosting**

Preparation:  
Ensure categorical variables are encoded appropriately (e.g., label encoding).  
Scaling is typically not required, but imputing missing values improves performance.  
can handle multicollinearity 

```{r}
#set.seed(547)
set.seed(123)
gbmGrid <- expand.grid(.n.trees = seq(100, 1000, by = 100), .interaction.depth = seq(1, 7, by = 2), .shrinkage = 0.01, .n.minobsinnode = c(5, 10, 15))
gbm_model <- train(ph ~ ., data = train_imputed, 
                   method = "gbm", 
                   preProc = c("center", "scale"),
                   tuneGrid = gbmGrid, 
                   trControl = control,
                   verbose = FALSE)

#shrinkage=0.01, interaction.depth=7, n.minobsinnode=10, n.trees=1000, RMSE=0.1116662, R2=0.5845802, MAE=0.08419827
gbm_tune <- gbm_model$results[gbm_model$results$n.trees == gbm_model$bestTune$n.trees & gbm_model$results$interaction.depth == gbm_model$bestTune$interaction.depth & gbm_model$results$n.minobsinnode == gbm_model$bestTune$n.minobsinnode,]
gbm_tune

gbm_predictions <- predict(gbm_model, newdata = test_imputed)

#RMSE=0.10565038, Rsquared=0.64276042, MAE= 0.07950506 
gbm_results <- postResample(pred = gbm_predictions, obs = test_imputed$ph)
gbm_results
```

Variable importance

mnf_flow         339.79
brand_code.C     120.07
usage_cont       108.89
oxygen_filler     87.21
alch_rel          80.17
temperature       73.41
pressure_vacuum   62.30
carb_pressure1    61.38
air_pressurer     58.16
bowl_setpoint     54.12
carb_rel          49.12
balling_lvl       46.28
balling           44.71
density           42.37
filler_speed      42.36
carb_flow         31.62
hyd_pressure3     25.90
pc_volume         24.38
fill_ounces       23.41
mfr               22.58

```{r}
importance_gbm <- varImp(gbm_model, scale = FALSE)
importance_gbm

plot(importance_gbm, top = 10, main = "top 10 predictors")
```



**Neural networks**

Preparation:  
Normalize or standardize numerical features.  
One-hot encode categorical variables.  
Impute missing values.  


```{r}
#set.seed(547)
set.seed(123)
nnetGrid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10, .bag = FALSE)

nnetModel <- train(ph ~ ., data = train_imputed, 
                   method = "avNNet",
                   preProc = c("center", "scale"),
                   tuneGrid = nnetGrid,
                   trControl = control,
                   linout = TRUE,
                   trace = FALSE)  

#size = 10, decay = 0 and bag = FALSE RMSE=0.1149139 R2=0.5559424 MAE=0.08557545
nnet_tune <- nnetModel$results[nnetModel$results$size == nnetModel$bestTune$size & nnetModel$results$decay == nnetModel$bestTune$decay, ]
nnet_tune

nnetPred <- predict(nnetModel, newdata = test_imputed)
#RMSE=0.11178875 R2=0.58921628 MAE=0.08249794 
nnet_results <- postResample(pred = nnetPred, obs = test_imputed$ph)
nnet_results

```

Variable importance

mnf_flow          0.22265
usage_cont        0.16234
bowl_setpoint     0.12216
filler_level      0.10591
pressure_setpoint 0.09704
carb_flow         0.08701
brand_code.C      0.08457
hyd_pressure3     0.05234
pressure_vacuum   0.05022
fill_pressure     0.04405
brand_code.D      0.03947
hyd_pressure2     0.03431
carb_rel          0.02556
oxygen_filler     0.02463
alch_rel          0.02198
temperature       0.02180
hyd_pressure4     0.02162
fill_ounces       0.01149
brand_code.B      0.01130
pc_volume         0.01039

```{r}
importance_nnet <- varImp(nnetModel, scale = FALSE)
importance_nnet

plot(importance_nnet, top = 10, main = "top 10 predictors")
```


**Support Vector Machine**

Preparation:  
Standardize numerical predictors to avoid dominance by variables with larger scales.  
Encode categorical variables using one-hot encoding.  

```{r}
#set.seed(547)
set.seed(123)
#Train SVM
svmGrid <- expand.grid(sigma = c(0.001, 0.01, 0.1), C = c(0.1, 1, 10, 100))
svmModel <- train(ph ~ ., data = train_featured, 
                  method = "svmRadial", 
                  preProc = c("center", "scale"),
                  tuneGrid = svmGrid,
                  trControl = control
                  )

#sigma = 0.1 C = 1 RMSE=0.1183132 R2=0.530464 MAE=0.08760546
svm_tune <- svmModel$results[svmModel$results$sigma == svmModel$bestTune$sigma & svmModel$results$C == svmModel$bestTune$C, ]
svm_tune

svmPred <- predict(svmModel, newdata = test_featured)
#RMSE=0.11812221 R2=0.54019858 MAE=0.08580952
svm_results <- postResample(pred = svmPred, obs = test_featured$ph)
svm_results
```

Variable importance

                   Overall
mnf_flow           0.22265
usage_cont         0.16234
setpoint_diff      0.13572
filler_level       0.10591
carb_flow          0.08701
brand_code.C       0.08457
carbonation_compos 0.06407
hyd_pressure3      0.05234
pressure_vacuum    0.05022
fill_pressure      0.04405
brand_code.D       0.03947
hyd_pressure2      0.03431
oxygen_filler      0.02463
temperature        0.02180
hyd_pressure4      0.02162
filler_efficiency  0.01658
gas_control_indx   0.01249
fill_ounces        0.01149
brand_code.B       0.01130
pc_volume          0.01039

```{r}
importance_svm <- varImp(svmModel, scale = FALSE)
importance_svm

plot(importance_svm, top = 10, main = "top 10 predictors")
```

**MARS**

```{r}
#set.seed(547) 
set.seed(123)
#Train MARS
marsGrid <- expand.grid(degree = 1:2, nprune = 2:20)

marsModel <- train(ph ~ ., data = train_featured, 
                   method = "earth",
                   preProc = c("center", "scale"),
                   tuneGrid = marsGrid,
                   trControl = control
)


#nprune = 20 degree = 2 RMSE=0.1263193 R2=0.4646403 MAE=0.09517493
mars_tune <- marsModel$results[marsModel$results$nprune == marsModel$bestTune$nprune & marsModel$results$degree == marsModel$bestTune$degree, ]
mars_tune

marsPred <- predict(marsModel, newdata = test_featured)
#RMSE=0.12625382 R2=0.47430313 MAE=0.09277686
mars_results <- postResample(pred = marsPred, obs = test_featured$ph)
mars_results
```


```{r}
#No new features
#set.seed(547) 
set.seed(123)
#Train MARS
marsGrid <- expand.grid(degree = 1:2, nprune = 2:20)

marsModel <- train(ph ~ ., data = train_imputed, 
                   method = "earth",
                   preProc = c("center", "scale"),
                   tuneGrid = marsGrid,
                   trControl = control
)


#nprune = 20 degree = 2 RMSE=0.1268247 R2=0.4605546 MAE=0.0968188
mars_tune <- marsModel$results[marsModel$results$nprune == marsModel$bestTune$nprune & marsModel$results$degree == marsModel$bestTune$degree, ]
mars_tune

marsPred <- predict(marsModel, newdata = test_imputed)
#RMSE=0.1236011 R2=0.4962139 MAE=0.0914482 
mars_results <- postResample(pred = marsPred, obs = test_imputed$ph)
mars_results
```

Variable importance   
mnf_flow         100.00
brand_code.C      75.72
hyd_pressure3     64.47
carb_pressure1    64.47
usage_cont        61.94
bowl_setpoint     58.09
pressure_vacuum   53.62
alch_rel          46.91
balling           43.22
temperature       38.22
air_pressurer     29.59
filler_speed      25.93
density           21.78
oxygen_filler     18.33

```{r}
p1 <- vip(marsModel, num_features = 40, geom = "point", value = "gcv") + ggtitle("GCV")
p2 <- vip(marsModel, num_features = 40, geom = "point", value = "rss") + ggtitle("RSS")

importance_mars <- varImp(marsModel, scale = FALSE)
importance_mars

gridExtra::grid.arrange(p1, p2, ncol = 2)
```


```{r}
set.seed(547) 
xgbGrid <- expand.grid(
  nrounds = seq(100, 700, by = 100),     # Broader range for boosting iterations
  eta = c(0.01, 0.1),             # Slightly broader learning rates
  max_depth = c(3, 5, 7),               # Broader range for tree depth
  gamma = c(0, 0.5, 1),                 # Broader gamma values for regularization
  colsample_bytree = c(0.7, 0.8, 1),    # Expanded values for column sampling
  min_child_weight = c(1, 3, 5),        # Broader range for child weight
  subsample = c(0.7, 0.8, 0.9)          # Broader range for subsampling
)

# Train the XGBoost model
xgb_model <- train(ph ~ ., data = train_imputed, 
  method = "xgbTree",
  preProc = c("center", "scale"), 
  tuneGrid = xgbGrid,
  trControl = control
)

#RMSE=0.1023946   Rsquared  =0.6527386      MAE =0.07495757
xgboost_tune <- xgb_model$results[xgb_model$results$max_depth == xgb_model$bestTune$max_depth & xgb_model$results$gamma == xgb_model$bestTune$gamma & xgb_model$results$nrounds == xgb_model$bestTune$nrounds & xgb_model$results$min_child_weight == xgb_model$bestTune$min_child_weight & xgb_model$results$eta == xgb_model$bestTune$eta, ]
xgboost_tune

# Predictions
xgb_predictions <- predict(xgb_model, newdata = test_imputed)

# RMSE=0.10554969   Rsquared  =0.60479526      MAE =0.07647919
  
xgb_results <- postResample(pred = xgb_predictions, obs = test_imputed$ph)
xgb_results
```

```{r}
stopCluster(cl)
```


# 4. Model Selection

```{r}
#Create empty df
results <- data.frame(
  Model = character(),
  Resample_RMSE = numeric(),
  Resample_R2 = numeric(),
  Resample_MAE = numeric(),
  Test_RMSE = numeric(),
  Test_R2 = numeric(),
  Test_MAE = numeric(),
  stringsAsFactors = FALSE
)

#Fill df with results
results <- rbind(results, data.frame(Model = "RF", Resample_RMSE = rf_tune$RMSE, Resample_R2 = rf_tune$Rsquared, Resample_MAE = rf_tune$MAE, Test_RMSE = rf_results[1], Test_R2 = rf_results[2], Test_MAE = rf_results[3]))
results <- rbind(results, data.frame(Model = "GBM", Resample_RMSE = gbm_tune$RMSE, Resample_R2 = gbm_tune$Rsquared, Resample_MAE = gbm_tune$MAE, Test_RMSE = gbm_results[1], Test_R2 = gbm_results[2], Test_MAE = gbm_results[3]))
results <- rbind(results, data.frame(Model = "NNet", Resample_RMSE = nnet_tune$RMSE, Resample_R2 = nnet_tune$Rsquared, Resample_MAE = nnet_tune$MAE, Test_RMSE = nnet_results[1], Test_R2 = nnet_results[2], Test_MAE = nnet_results[3]))
results <- rbind(results, data.frame(Model = "SVM", Resample_RMSE = svm_tune$RMSE, Resample_R2 = svm_tune$Rsquared, Resample_MAE = svm_tune$MAE, Test_RMSE = svm_results[1], Test_R2 = svm_results[2], Test_MAE = svm_results[3]))
results <- rbind(results, data.frame(Model = "MARS", Resample_RMSE = mars_tune$RMSE, Resample_R2 = mars_tune$Rsquared, Resample_MAE = mars_tune$MAE, Test_RMSE = mars_results[1], Test_R2 = mars_results[2], Test_MAE = mars_results[3]))
results <- rbind(results, data.frame(Model = "XGBoost", Resample_RMSE = xgboost_tune$RMSE, Resample_R2 = xgboost_tune$Rsquared, Resample_MAE = xgboost_tune$MAE, Test_RMSE = xgboost_results[1], Test_R2 = xgboost_results[2], Test_MAE = xgboost_results[3]))
row.names(results) <- NULL

results
```


Save results to excel file

```{r}
#Predictions for the evaluation dataset
eval_pred <- predict(best_model, newdata = eval_imputed)

#Create df
eval_results <- data.frame(
  ph_pred = eval_pred
)

eval_imputed_pred <- eval_imputed
eval_imputed_pred$ph_pred <- eval_pred

#Save to Excel
write_xlsx(eval_imputed_pred, "eval_imputed_pred.xlsx")
write_xlsx(eval_results, "ph_predictions.xlsx")
```


```{r}
write.csv(beverage_imputed,"beverage_imputed.csv", row.names = F)
write.csv(beverage_featured,"beverage_featured.csv", row.names = F)
```
# 5. Conclusion


# References

# Appendix: R Code

Analyze NAs for PH, they are all in brand B.

Analyze NAs for Brand code.

Do anything with brand B being represented the most?

Anything to do with the outliers?

There are binomial distributions, anything to do with them?

Do I need to use zeroVar to remove variables? VIF? PCA?

MFR remove?

MNF flow bs PH, values of -100, change that? Or just remove? The total number of observations sum to 1183, which represents a 46% of observations from the 2571.  

Not include Hyd Pressure 2, Hyd Pressure 3? There are two very near consecutive values, these are 0.0 and 0.2, the total number of observations sum to 882, which represents a 34.3% of observations from the 2571.



Analyze NAs for PH, they are all in brand B. What to do? Analyze NAs for Brand code. Do anything with brand B being represented the most? Anything to do with the outliers for PH or any other variables?  There are binomial distributions for some variable, check for what variables, anything to do with them?


```{r}
set.seed(123)

df_1 <- beverage_imputed %>%
  mutate(
    # Interaction among positively correlated features
    bowl_filler = bowl_setpoint * filler_level,
    bowl_pressure = bowl_setpoint * pressure_vacuum,
    filler_pressure = filler_level * pressure_vacuum,
    
    # Interaction between positive and negative correlations
    bowl_mnf_flow = bowl_setpoint * mnf_flow,
    filler_usage = filler_level * usage_cont,
    pressure_usage = pressure_vacuum * usage_cont,
    
    # Interaction among negatively correlated features
    mnf_usage = mnf_flow * usage_cont,
    mnf_pressure = mnf_flow * pressure_setpoint,
    
    # Ratios for potential nonlinear relationships
    pressure_temp_ratio = pressure_vacuum / temperature,
    filler_mnf_ratio = filler_level / mnf_flow
  )

correlations <- cor(df_1 %>% select(starts_with("bowl_"), starts_with("filler_"), ph), use = "complete.obs")
print(correlations["ph", ])

glm_model <- glm(
  ph ~ bowl_filler + bowl_pressure + filler_pressure +
       bowl_mnf_flow + filler_usage + pressure_usage +
       mnf_usage + mnf_pressure + pressure_temp_ratio + filler_mnf_ratio,
  data = df_1
)
summary(glm_model)

# Prepare data
interaction_data <- df_1 %>% select(ph, starts_with("bowl_"), starts_with("filler_"))
rf_model <- train(
  ph ~ ., data = interaction_data,
  method = "rf",
  importance = TRUE
)

# Variable importance
importance <- varImp(rf_model)
print(importance)
```

```{r}
df_1 <- df_1 %>%
  select(-bowl_setpoint, -filler_level, -bowl_filler, -bowl_pressure, -bowl_mnf_flow, -mnf_usage)
```



```{r}
set.seed(123)
train_df_idx <- createDataPartition(df_1$ph, p = 0.8, list = FALSE)
train_df <- df_1[train_df_idx, ]
test_df <- df_1[-train_df_idx, ]
```



```{r}
#set.seed(547)
set.seed(123)
rfGrid <- expand.grid(.mtry = c(2, 4, 6, 8, 10))
rf_model <- train(ph ~ ., data = train_df, 
                  method = "rf",
                  preProc = c("center", "scale"),
                  trControl = control,
                  tuneGrid = rfGrid,
                  importance = TRUE) 

#mtry=10, RMSE=0.1006935, R2=0.6761629, MAE=0.07335081
rf_tune <- rf_model$results[rf_model$results$mtry == rf_model$bestTune$mtry, ]
rf_tune

rf_pred <- predict(rf_model, newdata = test_df)

#RMSE=0.10009144, R2=0.68986540 MAE= 0.07124392 
rf_results <- postResample(pred = rf_pred, obs = test_df$ph)
rf_results

```


```{r}
#set.seed(547)
set.seed(123)
gbmGrid <- expand.grid(.n.trees = seq(100, 1000, by = 100), .interaction.depth = seq(1, 7, by = 2), .shrinkage = 0.01, .n.minobsinnode = c(5, 10, 15))
gbm_model <- train(ph ~ ., data = train_df, 
                   method = "gbm", 
                   preProc = c("center", "scale"),
                   tuneGrid = gbmGrid, 
                   trControl = control,
                   verbose = FALSE)

#shrinkage=0.01, interaction.depth=7, n.minobsinnode=10, n.trees=1000, RMSE=0.1111007, R2= 0.5883022, MAE=0.08336697
gbm_tune <- gbm_model$results[gbm_model$results$n.trees == gbm_model$bestTune$n.trees & gbm_model$results$interaction.depth == gbm_model$bestTune$interaction.depth & gbm_model$results$n.minobsinnode == gbm_model$bestTune$n.minobsinnode,]
gbm_tune

gbm_predictions <- predict(gbm_model, newdata = test_df)

#RMSE=0.10517869, Rsquared=0.64448922, MAE= 0.07763472  
gbm_results <- postResample(pred = gbm_predictions, obs = test_df$ph)
gbm_results
```

```{r}
plot(resid(rf_model), train_df$ph, xlab = "Residuals", ylab = "PH", main = "Residuals vs PH")

car::vif(glm(ph ~ ., data = train_df))
```

